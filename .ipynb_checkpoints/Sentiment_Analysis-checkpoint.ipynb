{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b399178",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8852a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aebf4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neattext.functions as nfx                       # for cleaning the text\n",
    "#from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Natural language toolkit\n",
    "from nltk.stem import PorterStemmer                          \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# For extraction of features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# For labelling the data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# For sentiment analysis depends on words \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451d9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import joblib\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643a77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69d4d056",
   "metadata": {},
   "source": [
    "### Get the english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfa8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"small_dataset/English_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c2ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 4 suspected cases of Covid19 in Kitui, test -ve.\n",
      "Though our #Hospitality industry is suffering right now, the takeaway sector is really busy.\n",
      "This course could set you apart from the rest as it includes a Level 2 qualification! ðŸ‘‰ https://t.co/A1I4vfLr0K @JobSkilla #InspiraForLife #OnlineCourses #Covid19\n",
      "Downward pressure on #FDI flows caused by the #coronavirus pandemic could range from -30% to -40% in 2020-21, significantly grimmer than previous projections and possibly worse than during the 2008 financial crisis, according to a report from \n",
      "@UNCTAD. (File Photo) https://t.co/X41fvvaBtM\n"
     ]
    }
   ],
   "source": [
    "for i in tweets.sample(3).Text:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40159fd4",
   "metadata": {},
   "source": [
    "#### Fill the **NAN** value to blank space or \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd2ec0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.fillna(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102069bc",
   "metadata": {},
   "source": [
    "# Cleaning and processing tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aee0a3",
   "metadata": {},
   "source": [
    "- Removing hashtag \n",
    "- Remove userhandles @\n",
    "- Removing Urls\n",
    "- Removing stopwords\n",
    "- Removing multiples spaces and special_characters\n",
    "- Removing numbers and dates\n",
    "- Convert into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4e9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    # removing retweets rt word and hashtag only\n",
    "    #remove_rt = lambda x : re.sub(r'^RT[\\s]+',\"\",x)\n",
    "    #tweets = tweets.map(remove_rt)\n",
    "    remove_hashtag = lambda x : re.sub(r'#', '', x)\n",
    "    tweets = tweets.map(remove_hashtag)\n",
    "    #tweets = tweets.drop_duplicates(keep=False)                #removing duplicates\n",
    "    tweets = tweets.apply(nfx.remove_userhandles)              #removing @\n",
    "    tweets = tweets.apply(nfx.remove_urls)                     #removing urls\n",
    "    tweets = tweets.apply(nfx.remove_stopwords)                #removing and the are etc.\n",
    "    tweets = tweets.apply(nfx.remove_special_characters)       #removing !^&#@($|)                #removing punc\n",
    "    tweets = tweets.apply(nfx.remove_multiple_spaces)          #removing multple_spaces\n",
    "    tweets = tweets.apply(nfx.remove_currencies)\n",
    "    tweets = tweets.apply(nfx.remove_numbers)\n",
    "    tweets = tweets.apply(nfx.remove_dates)\n",
    "    #tweets = tweets.apply(nfx.remove_emojis)\n",
    "    tweets = tweets.str.lower()\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37f81e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"Clean_text\"] = process_tweets(tweets[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095dcc50",
   "metadata": {},
   "source": [
    "#### Saving the clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8bdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_csv(\"en_clean_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf187652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebbcca1e",
   "metadata": {},
   "source": [
    "# Label the Dataset\n",
    "\n",
    "- Get the Features from Tfidfvectorizer\n",
    "- Then Cluster the tweets into 3 groups\n",
    "- Then get the sentiments of group to find out which group is positive, negative and neutral\n",
    "- Label the each tweet by sentiment analyzer and their cluster value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a9722816",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = TfidfVectorizer(min_df=5, max_df = 0.95, sublinear_tf=True, use_idf =True,max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "930f9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec1.fit_transform(tweets[\"Clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e994c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27aa9070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>access</th>\n",
       "      <th>according</th>\n",
       "      <th>account</th>\n",
       "      <th>accountable</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actions</th>\n",
       "      <th>...</th>\n",
       "      <th>yemenis</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>youre</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322549</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54214</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79349</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158745</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368119</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14075</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135413</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85908</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525723</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        able  about  absolutely  access  according  account  accountable  act  \\\n",
       "322549   0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "54214    0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "331204   0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "79349    0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "158745   0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "368119   0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "14075    0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "135413   0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "16955    0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "85908    0.0    0.0         0.0     0.0        0.0      0.0          0.0  0.0   \n",
       "\n",
       "        action  actions  ...  yemenis  yes  yesterday  yet  york       you  \\\n",
       "322549     0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "54214      0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "331204     0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "79349      0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "158745     0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "368119     0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "14075      0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "135413     0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "16955      0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.000000   \n",
       "85908      0.0      0.0  ...      0.0  0.0        0.0  0.0   0.0  0.525723   \n",
       "\n",
       "        young     youre  youtube  zero  \n",
       "322549    0.0  0.000000      0.0   0.0  \n",
       "54214     0.0  0.000000      0.0   0.0  \n",
       "331204    0.0  0.000000      0.0   0.0  \n",
       "79349     0.0  0.000000      0.0   0.0  \n",
       "158745    0.0  0.000000      0.0   0.0  \n",
       "368119    0.0  0.000000      0.0   0.0  \n",
       "14075     0.0  0.000000      0.0   0.0  \n",
       "135413    0.0  0.296804      0.0   0.0  \n",
       "16955     0.0  0.000000      0.0   0.0  \n",
       "85908     0.0  0.000000      0.0   0.0  \n",
       "\n",
       "[10 rows x 1500 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train,columns = vec1.get_feature_names()).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59494985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f38eff2",
   "metadata": {},
   "source": [
    "## clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aab642ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(n_clusters=3, init_size=4096, batch_size=4096,max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74c9a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "752bd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"cluster_labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff204b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean_text</th>\n",
       "      <th>cluster_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201327</th>\n",
       "      <td>trumplimit gatherings  people march</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147521</th>\n",
       "      <td>inside italys hospitals disturbing look corona...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227792</th>\n",
       "      <td>shoutouts competition</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21800</th>\n",
       "      <td>britons fear stranded morocco coronavirus trav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408160</th>\n",
       "      <td>failed america trumpliesaboutcoronavirus trump...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178735</th>\n",
       "      <td>people great things like numbers science truth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266000</th>\n",
       "      <td>ga coronavirus numbers rise  deaths  cases</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195026</th>\n",
       "      <td>im saying news trump hold rnc convention coron...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77829</th>\n",
       "      <td>ssot apparently managed convince entire souths...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97326</th>\n",
       "      <td>chris please shed light nyc la amp cities addr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Clean_text  cluster_labels\n",
       "201327              trumplimit gatherings  people march                 0\n",
       "147521  inside italys hospitals disturbing look corona...               1\n",
       "227792                              shoutouts competition               0\n",
       "21800   britons fear stranded morocco coronavirus trav...               1\n",
       "408160  failed america trumpliesaboutcoronavirus trump...               0\n",
       "178735  people great things like numbers science truth...               0\n",
       "266000         ga coronavirus numbers rise  deaths  cases               1\n",
       "195026  im saying news trump hold rnc convention coron...               1\n",
       "77829   ssot apparently managed convince entire souths...               2\n",
       "97326   chris please shed light nyc la amp cities addr...               2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweets.sample(10).get([\"Clean_text\",\"cluster_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00e940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4407f5a5",
   "metadata": {},
   "source": [
    "## finding sentiments of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb31c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sentiment(texts):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    c = 0\n",
    "    for text in texts:\n",
    "        #c += 1\n",
    "        #print(c)\n",
    "        score = analyzer.polarity_scores(text)\n",
    "        c += score[\"compound\"]\n",
    "    return c/len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "35d4095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.046592035087515526, -0.027193529358306174, 0.07625511002717983]\n"
     ]
    }
   ],
   "source": [
    "scores = [0,0,0]\n",
    "for i in range(3):\n",
    "    score = determine_sentiment(tweets[tweets[\"cluster_labels\"] == i].Clean_text)\n",
    "    scores[i] = score\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8ef9e719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 = 0.046592035087515526\n",
      "Cluster  1 = -0.027193529358306174\n",
      "Cluster  2 = 0.07625511002717983\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(scores):\n",
    "    print(\"Cluster \",i,\"=\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764ddcd",
   "metadata": {},
   "source": [
    "- Highest score gives **positive** sentiment\n",
    "- Lowest scores gives **negative** sentiment\n",
    "- Remaining cluster shows **Neutral** sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c325b",
   "metadata": {},
   "source": [
    "#### Saving the clustered tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77a1a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.to_csv(\"Clustered_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9239539",
   "metadata": {},
   "source": [
    "## label the tweets by sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f257b9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414545, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e85b2d",
   "metadata": {},
   "source": [
    "### Get the vaderSentiment of individual tweet and check there belonging cluser\n",
    "- If they match then add them to the final training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dafd8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "final_table = []\n",
    "for tweet,text,label in zip(tweets.Text,tweets.Clean_text,tweets.cluster_labels):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if (score[\"compound\"] > 0.5) and (label == 2) :              # text scores > 0.5 and belong to positive cluster\n",
    "        final_table.append([tweet,text,\"pos\",1])                 # text scores < 0.5 and belong to positive cluster\n",
    "    elif (score[\"compound\"] < 0.5) and (label == 1) :\n",
    "        final_table.append([tweet,text,\"neg\",-1])\n",
    "    elif (score[\"compound\"] <= 0.6) and (score[\"compound\"] >= 0.4) and (label == 0) : # text scores <= 0.6 and >= 0.3\n",
    "        final_table.append([tweet,text,\"neutral\",0])            #  belong to neutral cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1bf80b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_table,columns=[\"Tweets\",\"Clean_tweets\",\"labels\",\"Result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c7e95",
   "metadata": {},
   "source": [
    "## Counting the labels tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f13143ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweets = 26658\n",
      "Negative tweets = 103037\n",
      "Neutral tweets = 22556\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive tweets =\",df[df[\"Result\"]==1].shape[0])\n",
    "print(\"Negative tweets =\",df[df[\"Result\"]==-1].shape[0])\n",
    "print(\"Neutral tweets =\",df[df[\"Result\"]==0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b002ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaing training datasize = 152251\n"
     ]
    }
   ],
   "source": [
    "print(\"Total remaing training datasize =\",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d53a35b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eight major mobile carriers have agreed to share customer location data with the European Commission in a bid to track the spread of COVID-19, the GSMA said in a statement on Wednesday.\n",
      "#lockdown #COVID19 \n",
      "https://t.co/GqDJvgSOXX\n",
      "\n",
      "It is important to stay active every day while at home over #COVID19 outbreak. @WHO recommends at least 30 minutes #PhysicalActivity for adults and 60 minutes for children. #ActiveChallenge2020 #BeActive https://t.co/oTw1bLXOEC\n",
      "\n",
      "These are not usual times. \n",
      "Our 3D printers are ready to support. \n",
      "\n",
      "#COVID19 #Daimler https://t.co/f2uyzn67Jw\n",
      "\n",
      "Really appreciated this insightful and deeply human story from @AngelaKingKUOW. #COVID19\n",
      "\n",
      "https://t.co/SnjSyNNnwQ\n",
      "\n",
      "WEDNESDAY PODCAST: @VancouverFdn's @KevinMcCort, family doctor @DrMZeineddin, BC Today tech contributor and Vancouver Support . ca developer @AWSamuel discuss how people have been helping others amid the #COVID19 outbreak and the need. https://t.co/WORY4RBuWC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in df[df[\"labels\"]==\"pos\"].sample(5).Tweets:\n",
    "    print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be4ad66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"labelled_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eaede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10c5b13c",
   "metadata": {},
   "source": [
    "# Implementing Model \n",
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "523109b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size according to count of label\n",
    "X = df[df[\"Result\"] == -1][:60000]          #neg\n",
    "X = X.append(df[df[\"Result\"] == 1][:22000]) #pos\n",
    "X = X.append(df[df[\"Result\"] == 0][:22000]) #neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b2909e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df.sample(frac=1)    # mixing the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a3b18b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(training_data[\"Clean_tweets\"],training_data[\"Result\"], \n",
    "                                                 stratify=training_data[\"Result\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ae0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labelled_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb60719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Clean_tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Such news from SCMP, \\nso interesting https://...</td>\n",
       "      <td>news scmp interesting</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good move but should of been done a couple of ...</td>\n",
       "      <td>good couple weeks</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I hope these ventilators get built somehow. ht...</td>\n",
       "      <td>hope ventilators built somehow</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>.@scroll_in has been reporting extensively on ...</td>\n",
       "      <td>reporting extensively coronaviruscrisis follo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>This is fn preposterous. The richest country i...</td>\n",
       "      <td>fn preposterous richest country world covid ge...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             Tweets  \\\n",
       "0           0  Such news from SCMP, \\nso interesting https://...   \n",
       "1           1  Good move but should of been done a couple of ...   \n",
       "2           2  I hope these ventilators get built somehow. ht...   \n",
       "3           3  .@scroll_in has been reporting extensively on ...   \n",
       "4           4  This is fn preposterous. The richest country i...   \n",
       "\n",
       "                                        Clean_tweets   labels  Result  \n",
       "0                              news scmp interesting  neutral       0  \n",
       "1                                  good couple weeks  neutral       0  \n",
       "2                     hope ventilators built somehow  neutral       0  \n",
       "3   reporting extensively coronaviruscrisis follo...  neutral       0  \n",
       "4  fn preposterous richest country world covid ge...      pos       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca05752",
   "metadata": {},
   "source": [
    "getting x_train ,x_test features of their training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ffc7dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(max_features=2000)\n",
    "x = vec2.fit_transform(x_train).toarray()\n",
    "x_test = vec2.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9e22fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(x, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3a95c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8983613017634888"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc07b1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec2.transform(['I lost my job after the pandemic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8dc8c47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec2.transform(['I get my job after the pandemic i am happy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d7d3d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec2.transform(['I am sad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0218f5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec2.transform(['I am happy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "168b58c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec2.transform(['Getting food and working in life']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15786c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c8c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03431fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32e84b72",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a939371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "15f1cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model_Folder/vec.pkl']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving count vectorizer \n",
    "joblib.dump(vec2, 'Model_Folder/vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "066b6401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model_Folder/model.pkl']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model\n",
    "joblib.dump(model, 'Model_Folder/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f64941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e11f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
